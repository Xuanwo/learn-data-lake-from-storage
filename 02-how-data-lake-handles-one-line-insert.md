# How Data Lake Handles One Line Insert?

Welcome to the second chapter of Learn Data Lake From Storage! In this chapter, we will explore how data lakes handle one line insert. Please make sure you have finished the setup work for data lakes you want to explore.

## Apache Hive

> Visit [Apache Hive](lakes/apache-hive) to get it setup.

Let's insert some data:

```sql
INSERT INTO example_table VALUES (1, "a");
```

We will see:

```shell
:) sudo tree /var/lib/docker/volumes/apache-hive_warehouse/_data
/var/lib/docker/volumes/apache-hive_warehouse/_data
└── example_table
    └── 000000_0

2 directories, 1 file
```

A file is created for the data. The file name is generated by hive, and the content is parquet format.

Let's use `parquet_tools` to inspect it:

```shell
:) sudo parquet-tools show /var/lib/docker/volumes/apache-hive_warehouse/_data/example_table/000000_0

+------+--------+
|   id | name   |
|------+--------|
|    1 | a      |
+------+--------+

:) sudo parquet-tools inspect /var/lib/docker/volumes/apache-hive_warehouse/_data/example_table/000000_0

############ file meta data ############
created_by: parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)
num_columns: 2
num_rows: 1
num_row_groups: 1
format_version: 1.0
serialized_size: 415


############ Columns ############
id
name

############ Column(id) ############
name: id
path: id
max_definition_level: 1
max_repetition_level: 0
physical_type: INT32
logical_type: None
converted_type (legacy): NONE
compression: UNCOMPRESSED (space_saved: 0%)

############ Column(name) ############
name: name
path: name
max_definition_level: 1
max_repetition_level: 0
physical_type: BYTE_ARRAY
logical_type: String
converted_type (legacy): UTF8
compression: UNCOMPRESSED (space_saved: 0%)
```

### Summary

So hive only store data files in the storage layer, and all the metadata are stored in the hive metastore.

## Apache Iceberg

> Visit [Apache Iceberg](lakes/apache-iceberg) to get it setup.

Let's insert some data:

```sql
INSERT INTO demo.nyc.example_table VALUES (1, "a");
```

Let's see how storage changed:

```shell
2024-07-17 13:15          643  s3://warehouse/nyc/example_table/data/00000-0-b8b23675-d878-437c-a8f1-6b5b11cafeed-00001.parquet
2024-07-17 13:14         1207  s3://warehouse/nyc/example_table/metadata/00000-c8902826-08a4-443f-a4ae-d6d62c153816.metadata.json
2024-07-17 13:15         2239  s3://warehouse/nyc/example_table/metadata/00001-1ea6472c-98aa-4576-b66a-efbbf87dd354.metadata.json
2024-07-17 13:15         5773  s3://warehouse/nyc/example_table/metadata/631b4d7b-5501-4cb2-b205-7b8117a0fe7b-m0.avro
2024-07-17 13:15         3754  s3://warehouse/nyc/example_table/metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro
```

Apart from existing `metadata/00000-37121756-df65-47d6-93bb-b6eb53e33eea.metadata.json`, we have the following new files created after an insert:

- `data/00000-0-b8b23675-d878-437c-a8f1-6b5b11cafeed-00001.parquet`
- `metadata/00001-1ea6472c-98aa-4576-b66a-efbbf87dd354.metadata.json`
- `metadata/631b4d7b-5501-4cb2-b205-7b8117a0fe7b-m0.avro`
- `metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro`

### Metadata File

Let's checkout the new `metadata.json` file first:

```diff
5c5
<   "last-updated-ms" : 1721222050498,
---
>   "last-updated-ms" : 1721222138542,
53,55c53,79
<   "current-snapshot-id" : -1,
<   "refs" : { },
<   "snapshots" : [ ],
---
>   "current-snapshot-id" : 7304560488408846027,
>   "refs" : {
>     "main" : {
>       "snapshot-id" : 7304560488408846027,
>       "type" : "branch"
>     }
>   },
>   "snapshots" : [ {
>     "snapshot-id" : 7304560488408846027,
>     "timestamp-ms" : 1721222138542,
>     "summary" : {
>       "operation" : "append",
>       "spark.app.id" : "local-1721222015546",
>       "added-data-files" : "1",
>       "added-records" : "1",
>       "added-files-size" : "643",
>       "changed-partition-count" : "1",
>       "total-records" : "1",
>       "total-files-size" : "643",
>       "total-data-files" : "1",
>       "total-delete-files" : "0",
>       "total-position-deletes" : "0",
>       "total-equality-deletes" : "0"
>     },
>     "manifest-list" : "s3://warehouse/nyc/example_table/metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro",
>     "schema-id" : 0
>   } ],
57,58c81,88
<   "snapshot-log" : [ ],
<   "metadata-log" : [ ]
---
>   "snapshot-log" : [ {
>     "timestamp-ms" : 1721222138542,
>     "snapshot-id" : 7304560488408846027
>   } ],
>   "metadata-log" : [ {
>     "timestamp-ms" : 1721222050498,
>     "metadata-file" : "s3://warehouse/nyc/example_table/metadata/00000-c8902826-08a4-443f-a4ae-d6d62c153816.metadata.json"
>   } ]
```

Compared to the existing `metadata.json` file, we have the following changes:

- `snapshots` now contains a new snapshot, which records the operation, added data files, added records, etc.
- `snapshot-log` now contains the timestamp and snapshot id.
- `metadata-log` now contains the timestamp and last metadata file.

### Manifest List

The `manifest-list` points the snapshot files located at: `metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro`. 

Let's use `avro-tools` to inspect it:

```json
{
  "manifest_path" : "s3://warehouse/nyc/example_table/metadata/631b4d7b-5501-4cb2-b205-7b8117a0fe7b-m0.avro",
  "manifest_length" : 5773,
  "partition_spec_id" : 0,
  "added_snapshot_id" : {
    "long" : 7304560488408846027
  },
  "added_data_files_count" : {
    "int" : 1
  },
  "existing_data_files_count" : {
    "int" : 0
  },
  "deleted_data_files_count" : {
    "int" : 0
  },
  "partitions" : {
    "array" : [ ]
  },
  "added_rows_count" : {
    "long" : 1
  },
  "existing_rows_count" : {
    "long" : 0
  },
  "deleted_rows_count" : {
    "long" : 0
  }
}
```

The manifest list contains the following information:

- Path and size of the manifest file.
- Summary of the snapshot, including added data files, added records, etc.

### Manifest File

Manifest list will point to one or more manifest files which contains the following information:

```json
{
  "status" : 1,
  "snapshot_id" : {
    "long" : 7304560488408846027
  },
  "data_file" : {
    "file_path" : "s3://warehouse/nyc/example_table/data/00000-0-b8b23675-d878-437c-a8f1-6b5b11cafeed-00001.parquet",
    "file_format" : "PARQUET",
    "partition" : { },
    "record_count" : 1,
    "file_size_in_bytes" : 643,
    "block_size_in_bytes" : 67108864,
    "column_sizes" : {
      "array" : [ {
        "key" : 1,
        "value" : 46
      }, {
        "key" : 2,
        "value" : 48
      } ]
    },
    "value_counts" : {
      "array" : [ {
        "key" : 1,
        "value" : 1
      }, {
        "key" : 2,
        "value" : 1
      } ]
    },
    "null_value_counts" : {
      "array" : [ {
        "key" : 1,
        "value" : 0
      }, {
        "key" : 2,
        "value" : 0
      } ]
    },
    "nan_value_counts" : {
      "array" : [ ]
    },
    "lower_bounds" : {
      "array" : [ {
        "key" : 1,
        "value" : "\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000"
      }, {
        "key" : 2,
        "value" : "a"
      } ]
    },
    "upper_bounds" : {
      "array" : [ {
        "key" : 1,
        "value" : "\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000"
      }, {
        "key" : 2,
        "value" : "a"
      } ]
    },
    "key_metadata" : null,
    "split_offsets" : {
      "array" : [ 4 ]
    },
    "sort_order_id" : {
      "int" : 0
    }
  }
}
```

- `status` marks the status of the data file, could be `EXISTING`, `ADDED`, `DELETED`.
- `data_file` carries the information of the data file, including file path, file format, partition, record count, file size, etc.

### Data File

And finally, we will have the data file with value:

```shell
+------+--------+
|   id | name   |
|------+--------|
|    1 | a      |
+------+--------+
```

### Summary

So in iceberg table, one line insert is not just about adding data files, but also about updating the metadata files includes metadata file, manifest lists, manifest files, and data files.
