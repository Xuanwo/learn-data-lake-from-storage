# How Data Lake Handles One Line Insert?

Welcome to the second chapter of Learn Data Lake From Storage! In this chapter, we will explore how data lakes handle one line insert. Please make sure you have finished the setup work for data lakes you want to explore.

| Lake            | Storage                                                                           | 
|-----------------|-----------------------------------------------------------------------------------| 
| Apache Hive     | data files                                                                        |
| Apache Iceberg  | data files, `metadata.json`, manifest lists, manifest files                       |
| Apache Paimon   | data files, snapshot, {base,delta} manifest lists, manifest files, index manifest |

## Apache Hive

> Visit [Apache Hive](lakes/apache-hive) to get it setup.

Let's insert some data:

```sql
INSERT INTO example_table VALUES (1, "a");
```

We will see:

```shell
:) sudo tree /var/lib/docker/volumes/apache-hive_warehouse/_data
/var/lib/docker/volumes/apache-hive_warehouse/_data
└── example_table
    └── 000000_0

2 directories, 1 file
```

A file is created for the data. The file name is generated by hive, and the content is parquet format.

Let's use `parquet_tools` to inspect it:

```shell
:) sudo parquet-tools show /var/lib/docker/volumes/apache-hive_warehouse/_data/example_table/000000_0

+------+--------+
|   id | name   |
|------+--------|
|    1 | a      |
+------+--------+

:) sudo parquet-tools inspect /var/lib/docker/volumes/apache-hive_warehouse/_data/example_table/000000_0

############ file meta data ############
created_by: parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)
num_columns: 2
num_rows: 1
num_row_groups: 1
format_version: 1.0
serialized_size: 415


############ Columns ############
id
name

############ Column(id) ############
name: id
path: id
max_definition_level: 1
max_repetition_level: 0
physical_type: INT32
logical_type: None
converted_type (legacy): NONE
compression: UNCOMPRESSED (space_saved: 0%)

############ Column(name) ############
name: name
path: name
max_definition_level: 1
max_repetition_level: 0
physical_type: BYTE_ARRAY
logical_type: String
converted_type (legacy): UTF8
compression: UNCOMPRESSED (space_saved: 0%)
```

### Summary

So hive only store data files in the storage layer, and all the metadata are stored in the hive metastore.

## Apache Iceberg

> Visit [Apache Iceberg](lakes/apache-iceberg) to get it setup.

Let's insert some data:

```sql
INSERT INTO demo.nyc.example_table VALUES (1, 'a');
```

Let's see how storage changed:

```shell
2024-07-17 13:15          643  s3://warehouse/nyc/example_table/data/00000-0-b8b23675-d878-437c-a8f1-6b5b11cafeed-00001.parquet
2024-07-17 13:14         1207  s3://warehouse/nyc/example_table/metadata/00000-c8902826-08a4-443f-a4ae-d6d62c153816.metadata.json
2024-07-17 13:15         2239  s3://warehouse/nyc/example_table/metadata/00001-1ea6472c-98aa-4576-b66a-efbbf87dd354.metadata.json
2024-07-17 13:15         5773  s3://warehouse/nyc/example_table/metadata/631b4d7b-5501-4cb2-b205-7b8117a0fe7b-m0.avro
2024-07-17 13:15         3754  s3://warehouse/nyc/example_table/metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro
```

Apart from existing `metadata/00000-37121756-df65-47d6-93bb-b6eb53e33eea.metadata.json`, we have the following new files created after an insert:

- `data/00000-0-b8b23675-d878-437c-a8f1-6b5b11cafeed-00001.parquet`
- `metadata/00001-1ea6472c-98aa-4576-b66a-efbbf87dd354.metadata.json`
- `metadata/631b4d7b-5501-4cb2-b205-7b8117a0fe7b-m0.avro`
- `metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro`

### Metadata File

Let's checkout the new `metadata.json` file first:

```diff
5c5
<   "last-updated-ms" : 1721222050498,
---
>   "last-updated-ms" : 1721222138542,
53,55c53,79
<   "current-snapshot-id" : -1,
<   "refs" : { },
<   "snapshots" : [ ],
---
>   "current-snapshot-id" : 7304560488408846027,
>   "refs" : {
>     "main" : {
>       "snapshot-id" : 7304560488408846027,
>       "type" : "branch"
>     }
>   },
>   "snapshots" : [ {
>     "snapshot-id" : 7304560488408846027,
>     "timestamp-ms" : 1721222138542,
>     "summary" : {
>       "operation" : "append",
>       "spark.app.id" : "local-1721222015546",
>       "added-data-files" : "1",
>       "added-records" : "1",
>       "added-files-size" : "643",
>       "changed-partition-count" : "1",
>       "total-records" : "1",
>       "total-files-size" : "643",
>       "total-data-files" : "1",
>       "total-delete-files" : "0",
>       "total-position-deletes" : "0",
>       "total-equality-deletes" : "0"
>     },
>     "manifest-list" : "s3://warehouse/nyc/example_table/metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro",
>     "schema-id" : 0
>   } ],
57,58c81,88
<   "snapshot-log" : [ ],
<   "metadata-log" : [ ]
---
>   "snapshot-log" : [ {
>     "timestamp-ms" : 1721222138542,
>     "snapshot-id" : 7304560488408846027
>   } ],
>   "metadata-log" : [ {
>     "timestamp-ms" : 1721222050498,
>     "metadata-file" : "s3://warehouse/nyc/example_table/metadata/00000-c8902826-08a4-443f-a4ae-d6d62c153816.metadata.json"
>   } ]
```

Compared to the existing `metadata.json` file, we have the following changes:

- `snapshots` now contains a new snapshot, which records the operation, added data files, added records, etc.
- `snapshot-log` now contains the timestamp and snapshot id.
- `metadata-log` now contains the timestamp and last metadata file.

### Manifest List

The `manifest-list` points the snapshot files located at: `metadata/snap-7304560488408846027-1-631b4d7b-5501-4cb2-b205-7b8117a0fe7b.avro`. 

Let's use `avro-tools` to inspect it:

```json
{
  "manifest_path" : "s3://warehouse/nyc/example_table/metadata/631b4d7b-5501-4cb2-b205-7b8117a0fe7b-m0.avro",
  "manifest_length" : 5773,
  "partition_spec_id" : 0,
  "added_snapshot_id" : {
    "long" : 7304560488408846027
  },
  "added_data_files_count" : {
    "int" : 1
  },
  "existing_data_files_count" : {
    "int" : 0
  },
  "deleted_data_files_count" : {
    "int" : 0
  },
  "partitions" : {
    "array" : [ ]
  },
  "added_rows_count" : {
    "long" : 1
  },
  "existing_rows_count" : {
    "long" : 0
  },
  "deleted_rows_count" : {
    "long" : 0
  }
}
```

The manifest list contains the following information:

- Path and size of the manifest file.
- Summary of the snapshot, including added data files, added records, etc.

### Manifest File

Manifest list will point to one or more manifest files which contains the following information:

```json
{
  "status" : 1,
  "snapshot_id" : {
    "long" : 7304560488408846027
  },
  "data_file" : {
    "file_path" : "s3://warehouse/nyc/example_table/data/00000-0-b8b23675-d878-437c-a8f1-6b5b11cafeed-00001.parquet",
    "file_format" : "PARQUET",
    "partition" : { },
    "record_count" : 1,
    "file_size_in_bytes" : 643,
    "block_size_in_bytes" : 67108864,
    "column_sizes" : {
      "array" : [ {
        "key" : 1,
        "value" : 46
      }, {
        "key" : 2,
        "value" : 48
      } ]
    },
    "value_counts" : {
      "array" : [ {
        "key" : 1,
        "value" : 1
      }, {
        "key" : 2,
        "value" : 1
      } ]
    },
    "null_value_counts" : {
      "array" : [ {
        "key" : 1,
        "value" : 0
      }, {
        "key" : 2,
        "value" : 0
      } ]
    },
    "nan_value_counts" : {
      "array" : [ ]
    },
    "lower_bounds" : {
      "array" : [ {
        "key" : 1,
        "value" : "\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000"
      }, {
        "key" : 2,
        "value" : "a"
      } ]
    },
    "upper_bounds" : {
      "array" : [ {
        "key" : 1,
        "value" : "\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000"
      }, {
        "key" : 2,
        "value" : "a"
      } ]
    },
    "key_metadata" : null,
    "split_offsets" : {
      "array" : [ 4 ]
    },
    "sort_order_id" : {
      "int" : 0
    }
  }
}
```

- `status` marks the status of the data file, could be `EXISTING`, `ADDED`, `DELETED`.
- `data_file` carries the information of the data file, including file path, file format, partition, record count, file size, etc.

### Data File

And finally, we will have the data file with value:

```shell
+------+--------+
|   id | name   |
|------+--------|
|    1 | a      |
+------+--------+
```

### Summary

So in iceberg table, one line insert is not just about adding data files, but also about updating the metadata files includes metadata file, manifest lists, manifest files, and data files.

## Apache Paimon

> Visit [Apache Paimon](lakes/apache-paimon/README.md) to get it setup.

Let's insert some data:

```sql
INSERT INTO example_table VALUES (1, 'a');
```

Let's see how storage changed:

```shell
/var/lib/docker/volumes/apache-paimon_flink-data/_data/paimon
└── default.db
    └── example_table
        ├── bucket-0
        │   └── data-b6218035-8fc5-4c68-b22c-2fec2e4542d4-0.parquet
        ├── index
        │   └── index-c4d9fdb0-c92b-48d2-a919-70d9de36092c-0
        ├── manifest
        │   ├── index-manifest-55e3e815-08ab-4a70-a808-a8df5d275cb4-0
        │   ├── manifest-1d87a516-733b-40c4-870b-5b9d3515c0e5-0
        │   ├── manifest-list-4091b92b-01d3-4b91-8e47-8c9f61847d2f-0
        │   └── manifest-list-4091b92b-01d3-4b91-8e47-8c9f61847d2f-1
        ├── schema
        │   └── schema-0
        └── snapshot
            ├── EARLIEST
            ├── LATEST
            └── snapshot-1

8 directories, 10 files
```

We have the following new files created after an insert:

- `bucket-0/data-b6218035-8fc5-4c68-b22c-2fec2e4542d4-0.parquet`
- `index/index-c4d9fdb0-c92b-48d2-a919-70d9de36092c-0`
- `manifest/index-manifest-55e3e815-08ab-4a70-a808-a8df5d275cb4-0`
- `manifest/manifest-1d87a516-733b-40c4-870b-5b9d3515c0e5-0`
- `manifest/manifest-list-4091b92b-01d3-4b91-8e47-8c9f61847d2f-0`
- `manifest/manifest-list-4091b92b-01d3-4b91-8e47-8c9f61847d2f-1`
- `snapshot/snapshot-1`
- `snapshot/EARLIEST`
- `snapshot/LATEST`

Let's copy the entire `paimon` data dir locally and inspect the files.

### Snapshot

Snapshot is the entry point of the paimon table. Every write operation will create a new snapshot. `EARLIEST` and `LATEST` will point to the earliest and latest snapshot respectively for the reader to find the snapshot easier.

Snapshot also stored as JSON file in storage layer:

```json
{
  "version" : 3,
  "id" : 1,
  "schemaId" : 0,
  "baseManifestList" : "manifest-list-4091b92b-01d3-4b91-8e47-8c9f61847d2f-0",
  "deltaManifestList" : "manifest-list-4091b92b-01d3-4b91-8e47-8c9f61847d2f-1",
  "changelogManifestList" : null,
  "indexManifest" : "index-manifest-55e3e815-08ab-4a70-a808-a8df5d275cb4-0",
  "commitUser" : "cf568e07-05ad-4943-b4bd-37461bc58729",
  "commitIdentifier" : 9223372036854775807,
  "commitKind" : "APPEND",
  "timeMillis" : 1721287833568,
  "logOffsets" : { },
  "totalRecordCount" : 1,
  "deltaRecordCount" : 1,
  "changelogRecordCount" : 0,
  "watermark" : -9223372036854775808
}
```

In the snapshot, we have the following information:

- `baseManifestList` and `deltaManifestList` point to the manifest list.
- `indexManifest` points to the index file.

There is a `changeLogManifestList` field, but it is not used in this case. We will revisit this part in the future.

We can infer that for every write operation, Paimon creates two manifest lists: one for the base and one for the delta. Thus, consumers can read data from the base manifest list and apply the delta manifest list to access the latest data or simply consume the delta portion of the data.

### Manifest List

The base manifest list is empty, as expected, given that our last state was an empty table. The delta manifest contains the following data:

```json
{
  "org.apache.paimon.avro.generated.record": {
    "_VERSION": 2,
    "_FILE_NAME": "manifest-1d87a516-733b-40c4-870b-5b9d3515c0e5-0",
    "_FILE_SIZE": 1862,
    "_NUM_ADDED_FILES": 1,
    "_NUM_DELETED_FILES": 0,
    "_PARTITION_STATS": {
      "org.apache.paimon.avro.generated.record__PARTITION_STATS": {
        "_MIN_VALUES": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "_MAX_VALUES": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "_NULL_COUNTS": {
          "array": []
        }
      }
    },
    "_SCHEMA_ID": 0
  }
}
```

We have the path to the manifest file, the file size, the number of added and deleted files, and the partition stats.

### Manifest File

The manifest file contains the following data:

```json
{
  "org.apache.paimon.avro.generated.record": {
    "_VERSION": 2,
    "_KIND": 0,
    "_PARTITION": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
    "_BUCKET": 0,
    "_TOTAL_BUCKETS": -1,
    "_FILE": {
      "org.apache.paimon.avro.generated.record__FILE": {
        "_FILE_NAME": "data-b6218035-8fc5-4c68-b22c-2fec2e4542d4-0.parquet",
        "_FILE_SIZE": 1004,
        "_ROW_COUNT": 1,
        "_MIN_KEY": "\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "_MAX_KEY": "\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "_KEY_STATS": {
          "org.apache.paimon.avro.generated.record__FILE__KEY_STATS": {
            "_MIN_VALUES": "\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
            "_MAX_VALUES": "\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
            "_NULL_COUNTS": {
              "array": [
                {
                  "long": 0
                }
              ]
            }
          }
        },
        "_VALUE_STATS": {
          "org.apache.paimon.avro.generated.record__FILE__VALUE_STATS": {
            "_MIN_VALUES": "\u0000\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000a\u0000\u0000\u0000\u0000\u0000\u0000",
            "_MAX_VALUES": "\u0000\u0000\u0000\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000a\u0000\u0000\u0000\u0000\u0000\u0000",
            "_NULL_COUNTS": {
              "array": [
                {
                  "long": 0
                },
                {
                  "long": 0
                }
              ]
            }
          }
        },
        "_MIN_SEQUENCE_NUMBER": 0,
        "_MAX_SEQUENCE_NUMBER": 0,
        "_SCHEMA_ID": 0,
        "_LEVEL": 0,
        "_EXTRA_FILES": [],
        "_CREATION_TIME": {
          "long": 1721287833375
        },
        "_DELETE_ROW_COUNT": {
          "long": 0
        },
        "_EMBEDDED_FILE_INDEX": null,
        "_FILE_SOURCE": {
          "int": 0
        }
      }
    }
  }
}
```

Apart from data files path and size, manifest will also have key stats and value stats, which contains the min and max values of the key and value columns.

### Data File

And finally, we will have the data file with value:

```shell
+-----------+--------------------+---------------+------+--------+
|   _KEY_id |   _SEQUENCE_NUMBER |   _VALUE_KIND |   id | name   |
|-----------+--------------------+---------------+------+--------|
|         1 |                  0 |             0 |    1 | a      |
+-----------+--------------------+---------------+------+--------+
```

Paimon will store some extra information in the data file, including key, sequence number, value kind, etc.

### Index Manifest

Part from data and it's related stats, paimon will also write index manifest. We will revisit this part in the future.

### Summary

So in Paimon, one line insert is not just about adding data files, but also about updating the metadata files includes snapshot, {base,delta} manifest lists, index manifest, manifest files, and data files.